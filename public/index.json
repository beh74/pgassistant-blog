[{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":""},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"PostgreSQL Autovacuum Tuning Based on Table Size This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin). Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nWITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_scale_factor=%\u0026rsquo; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_threshold=%\u0026rsquo; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_scale_factor=%\u0026rsquo; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_threshold=%\u0026rsquo; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026lsquo;r\u0026rsquo; AND n.nspname NOT IN (\u0026lsquo;pg_catalog\u0026rsquo;, \u0026lsquo;information_schema\u0026rsquo;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026lsquo;.\u0026rsquo; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026lsquo;ANALYZE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026lsquo;;\u0026rsquo; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026lsquo;ALTER TABLE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026rsquo; SET (\u0026rsquo; || \u0026lsquo;autovacuum_vacuum_scale_factor = \u0026rsquo; || new_vacuum_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_vacuum_threshold = \u0026rsquo; || new_vacuum_threshold || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_scale_factor = \u0026rsquo; || new_analyze_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_threshold = \u0026rsquo; || new_analyze_threshold || \u0026lsquo;);\u0026rsquo; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST;\n","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"postgresql-autovacuum-tuning-based-on-table-size\"\u003ePostgreSQL Autovacuum Tuning Based on Table Size\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":""},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin). Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nWITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_scale_factor=%\u0026rsquo; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_threshold=%\u0026rsquo; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_scale_factor=%\u0026rsquo; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_threshold=%\u0026rsquo; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026lsquo;r\u0026rsquo; AND n.nspname NOT IN (\u0026lsquo;pg_catalog\u0026rsquo;, \u0026lsquo;information_schema\u0026rsquo;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026lsquo;.\u0026rsquo; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026lsquo;ANALYZE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026lsquo;;\u0026rsquo; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026lsquo;ALTER TABLE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026rsquo; SET (\u0026rsquo; || \u0026lsquo;autovacuum_vacuum_scale_factor = \u0026rsquo; || new_vacuum_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_vacuum_threshold = \u0026rsquo; || new_vacuum_threshold || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_scale_factor = \u0026rsquo; || new_analyze_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_threshold = \u0026rsquo; || new_analyze_threshold || \u0026lsquo;);\u0026rsquo; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST;\n","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin). Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nWITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_scale_factor=%\u0026rsquo; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_vacuum_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_vacuum_threshold=%\u0026rsquo; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_scale_factor=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_scale_factor=%\u0026rsquo; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026lsquo;autovacuum_analyze_threshold=\u0026rsquo;, \u0026lsquo;\u0026rsquo;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026lsquo;autovacuum_analyze_threshold=%\u0026rsquo; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026lsquo;r\u0026rsquo; AND n.nspname NOT IN (\u0026lsquo;pg_catalog\u0026rsquo;, \u0026lsquo;information_schema\u0026rsquo;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026lsquo;.\u0026rsquo; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026lsquo;ANALYZE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026lsquo;;\u0026rsquo; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026lsquo;ALTER TABLE \u0026rsquo; || quote_ident(nspname) || \u0026lsquo;.\u0026rsquo; || quote_ident(relname) || \u0026rsquo; SET (\u0026rsquo; || \u0026lsquo;autovacuum_vacuum_scale_factor = \u0026rsquo; || new_vacuum_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_vacuum_threshold = \u0026rsquo; || new_vacuum_threshold || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_scale_factor = \u0026rsquo; || new_analyze_scale || \u0026lsquo;, \u0026rsquo; || \u0026lsquo;autovacuum_analyze_threshold = \u0026rsquo; || new_analyze_threshold || \u0026lsquo;);\u0026rsquo; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST;\n","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin). Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nThe query WITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_scale_factor=%\u0026#39; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_threshold=%\u0026#39; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_scale_factor=%\u0026#39; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_threshold=%\u0026#39; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND n.nspname NOT IN (\u0026#39;pg_catalog\u0026#39;, \u0026#39;information_schema\u0026#39;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026#39;.\u0026#39; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026#39;ANALYZE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39;;\u0026#39; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026#39;ALTER TABLE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39; SET (\u0026#39; || \u0026#39;autovacuum_vacuum_scale_factor = \u0026#39; || new_vacuum_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_vacuum_threshold = \u0026#39; || new_vacuum_threshold || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_scale_factor = \u0026#39; || new_analyze_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_threshold = \u0026#39; || new_analyze_threshold || \u0026#39;);\u0026#39; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST; ","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin) or search for query \u0026lsquo;vacuum\u0026rsquo; in pgAssistant. Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nThe query WITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_scale_factor=%\u0026#39; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_threshold=%\u0026#39; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_scale_factor=%\u0026#39; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_threshold=%\u0026#39; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND n.nspname NOT IN (\u0026#39;pg_catalog\u0026#39;, \u0026#39;information_schema\u0026#39;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026#39;.\u0026#39; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026#39;ANALYZE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39;;\u0026#39; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026#39;ALTER TABLE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39; SET (\u0026#39; || \u0026#39;autovacuum_vacuum_scale_factor = \u0026#39; || new_vacuum_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_vacuum_threshold = \u0026#39; || new_vacuum_threshold || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_scale_factor = \u0026#39; || new_analyze_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_threshold = \u0026#39; || new_analyze_threshold || \u0026#39;);\u0026#39; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST; ","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin) or search for query \u0026lsquo;vacuum\u0026rsquo; in pgAssistant. Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nThe query WITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_scale_factor=%\u0026#39; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_threshold=%\u0026#39; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_scale_factor=%\u0026#39; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_threshold=%\u0026#39; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND n.nspname NOT IN (\u0026#39;pg_catalog\u0026#39;, \u0026#39;information_schema\u0026#39;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026#39;.\u0026#39; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026#39;ANALYZE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39;;\u0026#39; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026#39;ALTER TABLE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39; SET (\u0026#39; || \u0026#39;autovacuum_vacuum_scale_factor = \u0026#39; || new_vacuum_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_vacuum_threshold = \u0026#39; || new_vacuum_threshold || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_scale_factor = \u0026#39; || new_analyze_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_threshold = \u0026#39; || new_analyze_threshold || \u0026#39;);\u0026#39; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST; ","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"},{"content":"Object This post provides a SQL query that analyzes PostgreSQL tables and suggests optimized autovacuum settings on a per-table basis. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to table size and usage patterns.\nGoal PostgreSQL uses autovacuum to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\nTable bloat (too much dead data) Poor query planning (outdated statistics) Unnecessary I/O overhead (frequent vacuums on small tables) This script identifies tables with suboptimal autovacuum settings and suggests tailored values using industry recommendations.\nLogic Used The query calculates optimal values for four key autovacuum parameters:\nautovacuum_vacuum_scale_factor autovacuum_vacuum_threshold autovacuum_analyze_scale_factor autovacuum_analyze_threshold These are determined based on the estimated number of rows in each table (pg_class.reltuples).\nDynamic Threshold Logic Estimated Row Count (reltuples) vacuum_scale_factor vacuum_threshold analyze_scale_factor analyze_threshold \u0026gt; 1,000,000 0.0 400,000 0.0 100,000 100,000 – 1,000,000 0.005 10,000 0.01 10,000 10,000 – 100,000 0.02 1,000 0.01 1,000 \u0026lt; 10,000 0.01 1,000 0.005 500 These values are based on community best practices from sources like:\nEnterpriseDB Percona pganalyze Keith Fiske Special Cases If reltuples = -1, it means the table has never been analyzed. → The script will suggest: ANALYZE schema.table; If a table already has optimal settings, no action is suggested. Output The query returns:\nTable name Current and suggested autovacuum values A SQL command: either ANALYZE or ALTER TABLE ... SET (...) You can filter on suggested_action IS NOT NULL to extract only actionable commands.\nHow to Use Run the query in a PostgreSQL-compatible tool (e.g. psql, DBeaver, pgAdmin) or search for query \u0026lsquo;vacuum\u0026rsquo; in pgAssistant. Review the suggested_action column. Apply the ALTER TABLE and ANALYZE commands where appropriate. ⚠️Always test configuration changes in staging environments before deploying to production.\nNotes The script assumes default PostgreSQL configuration as baseline (scale_factor = 0.2, threshold = 50). It is designed for PostgreSQL 12+. It ignores pg_catalog and information_schema namespaces. Feedback Feel free to contribute improvements or report issues if your workload requires more specialized tuning (e.g. append-only tables, partitioned workloads, etc.).\nThe query WITH table_info AS ( SELECT n.nspname, c.relname, c.oid, COALESCE(c.reltuples, -1) AS reltuples, c.reloptions, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_scale_factor=%\u0026#39; )::float AS current_vacuum_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_vacuum_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_vacuum_threshold=%\u0026#39; )::int AS current_vacuum_threshold, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_scale_factor=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_scale_factor=%\u0026#39; )::float AS current_analyze_scale, ( SELECT regexp_replace(opt, \u0026#39;autovacuum_analyze_threshold=\u0026#39;, \u0026#39;\u0026#39;) FROM unnest(c.reloptions) opt WHERE opt LIKE \u0026#39;autovacuum_analyze_threshold=%\u0026#39; )::int AS current_analyze_threshold FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND n.nspname NOT IN (\u0026#39;pg_catalog\u0026#39;, \u0026#39;information_schema\u0026#39;) ), computed_autovacuum AS ( SELECT *, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.005 WHEN reltuples \u0026gt; 10000 THEN 0.02 WHEN reltuples \u0026gt;= 0 THEN 0.01 ELSE NULL END AS new_vacuum_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 400000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 1000 ELSE NULL END AS new_vacuum_threshold, CASE WHEN reltuples \u0026gt; 1000000 THEN 0.0 WHEN reltuples \u0026gt; 100000 THEN 0.01 WHEN reltuples \u0026gt; 10000 THEN 0.01 WHEN reltuples \u0026gt;= 0 THEN 0.005 ELSE NULL END AS new_analyze_scale, CASE WHEN reltuples \u0026gt; 1000000 THEN 100000 WHEN reltuples \u0026gt; 100000 THEN 10000 WHEN reltuples \u0026gt; 10000 THEN 1000 WHEN reltuples \u0026gt;= 0 THEN 500 ELSE NULL END AS new_analyze_threshold FROM table_info ) SELECT nspname || \u0026#39;.\u0026#39; || relname AS table_name, reltuples AS estimated_rows, COALESCE(current_vacuum_scale, 0.2) AS current_vacuum_scale, COALESCE(current_vacuum_threshold, 50) AS current_vacuum_threshold, COALESCE(current_analyze_scale, 0.1) AS current_analyze_scale, COALESCE(current_analyze_threshold, 50) AS current_analyze_threshold, new_vacuum_scale, new_vacuum_threshold, new_analyze_scale, new_analyze_threshold, CASE WHEN reltuples = -1 THEN \u0026#39;ANALYZE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39;;\u0026#39; WHEN COALESCE(current_vacuum_scale, 0.2) != new_vacuum_scale OR COALESCE(current_vacuum_threshold, 50) != new_vacuum_threshold OR COALESCE(current_analyze_scale, 0.1) != new_analyze_scale OR COALESCE(current_analyze_threshold, 50) != new_analyze_threshold THEN \u0026#39;ALTER TABLE \u0026#39; || quote_ident(nspname) || \u0026#39;.\u0026#39; || quote_ident(relname) || \u0026#39; SET (\u0026#39; || \u0026#39;autovacuum_vacuum_scale_factor = \u0026#39; || new_vacuum_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_vacuum_threshold = \u0026#39; || new_vacuum_threshold || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_scale_factor = \u0026#39; || new_analyze_scale || \u0026#39;, \u0026#39; || \u0026#39;autovacuum_analyze_threshold = \u0026#39; || new_analyze_threshold || \u0026#39;);\u0026#39; ELSE NULL END AS suggested_action FROM computed_autovacuum ORDER BY reltuples DESC NULLS LAST; ","permalink":"http://localhost:1313/post/vaccum/","summary":"\u003ch1 id=\"object\"\u003eObject\u003c/h1\u003e\n\u003cp\u003eThis post provides a SQL query that analyzes PostgreSQL tables and suggests optimized \u003ccode\u003eautovacuum\u003c/code\u003e settings on a \u003cstrong\u003eper-table basis\u003c/strong\u003e. It aims to improve database health and performance by adapting vacuum/analyze thresholds according to \u003cstrong\u003etable size\u003c/strong\u003e and \u003cstrong\u003eusage patterns\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"goal\"\u003eGoal\u003c/h2\u003e\n\u003cp\u003ePostgreSQL uses \u003ccode\u003eautovacuum\u003c/code\u003e to automatically clean up dead tuples and refresh planner statistics. However, the default settings may be too aggressive for small tables or too lax for large ones, leading to:\u003c/p\u003e","title":"PostgreSQL Autovacuum Tuning Based on Table Size"},{"content":"Privacy Data privacy is no longer a luxury — it’s a necessity.\nToday, we’ll show you how to use pgAssistant with Infomaniak Cloud — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\nWhy Data Sovereignty Matters When using AI tools in the cloud, where your data is processed and stored has a major impact:\nLegal compliance (e.g. GDPR) Data residency requirements Protection from mass surveillance Infomaniak’s Swiss Cloud is one of the few solutions that:\nIs fully hosted in Switzerland Operates under strict Swiss privacy laws Is ISO 27001 / GDPR compliant Powered by renewable energy Perfect match for privacy-conscious developers and European companies.\nHow to use my Infomaniak account with pgAssistant I recommend to use the llama3 model with pgAssistant, which gives good results.\nThe Environment variable LOCAL_LLM_URI must contains your product ID like this : https://api.infomaniak.com/1/ai//openai\nThe Environment variable OPENAI_API_KEY is the value of your token API.\ndocker-compose.yml file with Infomaniak account services: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=fGIDaOIfn-xxxxx - OPENAI_API_MODEL=llama3 - LOCAL_LLM_URI=https://api.infomaniak.com/1/ai/108043/openai - SECRET_KEY=bertrand ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json Sample result with table definition helper Lets analyze this DDL with llama3 hosted by Infomaniak :\nand here is the result :\nConclusion If you don’t have the resources to set up an on-premise infrastructure capable of running open-source LLMs, but data privacy still matters to you, then Infomaniak’s sovereign cloud is definitely worth considering.\nI’ve been using it personally for several months and I’m genuinely impressed. For me, it checks all the right boxes:\n✅ Strong privacy guarantees\n✅ Affordable pricing (my pricing this month for Input tokens : 1082936, Output tokens : 123411 = 1.82 EUR. With OpenAI, estimated price : 15 EUR)\n✅ Solid performance\nJust to be clear: I have no commercial relationship with Infomaniak — this is simply a personal recommendation based on real usage.\n","permalink":"http://localhost:1313/post/pgassistant-on-swissdata/","summary":"\u003ch1 id=\"privacy\"\u003ePrivacy\u003c/h1\u003e\n\u003cp\u003eData privacy is no longer a luxury — it’s a necessity.\u003c/p\u003e\n\u003cp\u003eToday, we’ll show you how to use pgAssistant with \u003ca href=\"https://swissdata.ai/en\"\u003eInfomaniak Cloud\u003c/a\u003e — Infomaniak\u0026rsquo;s privacy-focused, sovereign cloud solution based in Switzerland🇨.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-data-sovereignty-matters\"\u003eWhy Data Sovereignty Matters\u003c/h2\u003e\n\u003cp\u003eWhen using AI tools in the cloud, where your data is processed and stored has a major impact:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLegal compliance (e.g. GDPR)\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData residency requirements\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProtection from mass surveillance\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInfomaniak’s Swiss Cloud is one of the few solutions that:\u003c/p\u003e","title":"Using pgAssistant with Infomaniak’s AI Cloud"},{"content":"How to do it pg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find here the official Postgres documentation.\nTo enable this module, add this option on the command that runs Posgres :\nshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; Then, connect to the database and run this command :\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements; (When you run pgAssistant, he will execute this SQL Statements)\nIf you run Postgresql in a docker environment Here is a sample docker-compose file that enables the module :\nservices: demo-db: restart: always image: postgres:17 environment: - POSTGRES_USER=demo - POSTGRES_PASSWORD=demo - POSTGRES_DB=demo command: \u0026gt; postgres -c shared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39; -c max_connections=200 -c shared_buffers=\u0026#39;512MB\u0026#39; -c effective_cache_size=\u0026#39;1536MB\u0026#39; -c maintenance_work_mem=\u0026#39;128MB\u0026#39; -c checkpoint_completion_target=0.9 -c wal_buffers=\u0026#39;16MB\u0026#39; -c default_statistics_target=100 -c random_page_cost=1.1 -c effective_io_concurrency=200 -c work_mem=\u0026#39;1310kB\u0026#39; -c huge_pages=\u0026#39;off\u0026#39; -c min_wal_size=\u0026#39;1GB\u0026#39; -c max_wal_size=\u0026#39;4GB\u0026#39; -c max_worker_processes=4 -c max_parallel_workers_per_gather=2 -c max_parallel_workers=4 -c max_parallel_maintenance_workers=2 ports: - 5432:5432 ","permalink":"http://localhost:1313/doc/pg_stat_statments/","summary":"\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003epg_stat_statements is required by pgAssistant. If you are not familiar with this module, you can find \u003ca href=\"https://www.postgresql.org/docs/current/pgstatstatements.html\"\u003ehere\u003c/a\u003e the official Postgres documentation.\u003c/p\u003e\n\u003cp\u003eTo enable this module, add this option on the command that runs Posgres :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eshared_preload_libraries=\u0026#39;pg_stat_statements\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThen, connect to the database and run this command :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e(When you run pgAssistant, he will execute this SQL Statements)\u003c/p\u003e\n\u003ch1 id=\"if-you-run-postgresql-in-a-docker-environment\"\u003eIf you run Postgresql in a docker environment\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose file that enables the module :\u003c/p\u003e","title":"Enable pg_stat_statements module"},{"content":"Before you begin You must enable the pg_stat_statements module on your postgres database. Here is a documentation\nUsing the NexSol Technologies docker file Here is a sample docker-compose.yml file to run pgassistant :\nservices: pgassistant: image: nexsoltech/pgassistant:latest restart: always environment: - OPENAI_API_KEY=nothing - OPENAI_API_MODEL=codestral:latest - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/ - SECRET_KEY=mySecretKey4PgAssistant ports: - \u0026#34;8080:5005\u0026#34; volumes: - ./myqueries.json:/home/pgassistant/myqueries.json The file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc here\nEnvrionment variables Variable Description Example value OPENAI_API_KEY Dummy key (required by clients expecting a token) nothing OPENAI_API_MODEL Model identifier to use with the API codestral:latest or mistral:latest LOCAL_LLM_URI Local endpoint URL for the OpenAI-compatible API http://host.docker.internal:11434/v1/ SECRET_KEY Used to encrypt some htttp session variables. mySecretKey4PgAssistant Notes OPENAI_API_KEY is required by most clients but not used when querying local LLMs like Ollama. You can set it to any placeholder (e.g. nothing). OPENAI_API_MODEL must match the model name loaded in Ollama (e.g. codestral, llama3, mistral, etc.). LOCAL_LLM_URI should point to the Ollama server, accessible from inside your Docker container via host.docker.internal. How to build your docker image Simply clone the repo and then build your own image like this :\ngit clone https://github.com/nexsol-technologies/pgassistant.git cd pgassistant docker build . -t mypgassistant:1.0 ","permalink":"http://localhost:1313/doc/startup_docker/","summary":"\u003ch1 id=\"before-you-begin\"\u003eBefore you begin\u003c/h1\u003e\n\u003cp\u003eYou must enable the \u003cstrong\u003epg_stat_statements\u003c/strong\u003e module on your postgres database. \u003ca href=\"/doc/pg_stat_statments\"\u003eHere is a documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"using-the-nexsol-technologies-docker-file\"\u003eUsing the NexSol Technologies docker file\u003c/h1\u003e\n\u003cp\u003eHere is a sample docker-compose.yml file to run pgassistant :\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eservices:\n  pgassistant:\n    image: nexsoltech/pgassistant:latest\n    restart: always\n    environment:\n      - OPENAI_API_KEY=nothing\n      - OPENAI_API_MODEL=codestral:latest\n      - LOCAL_LLM_URI=http://host.docker.internal:11434/v1/\n      - SECRET_KEY=mySecretKey4PgAssistant\n    ports:\n      - \u0026#34;8080:5005\u0026#34;\n    volumes:\n      - ./myqueries.json:/home/pgassistant/myqueries.json\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe file myqueries.json is not necessary to run pgAssistant, but it should be usefull. Please read the doc \u003ca href=\"/doc/myqueries\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"envrionment-variables\"\u003eEnvrionment variables\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVariable\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n          \u003cth\u003eExample value\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eDummy key (required by clients expecting a token)\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003enothing\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eModel identifier to use with the API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ecodestral:latest\u003c/code\u003e or \u003ccode\u003emistral:latest\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eLocal endpoint URL for the OpenAI-compatible API\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ehttp://host.docker.internal:11434/v1/\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eSECRET_KEY\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eUsed to encrypt some htttp session variables.\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003emySecretKey4PgAssistant\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"notes\"\u003eNotes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_KEY\u003c/code\u003e is required by most clients but not used when querying local LLMs like \u003cstrong\u003eOllama\u003c/strong\u003e. You can set it to any placeholder (e.g. \u003ccode\u003enothing\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eOPENAI_API_MODEL\u003c/code\u003e must match the model name loaded in Ollama (e.g. \u003ccode\u003ecodestral\u003c/code\u003e, \u003ccode\u003ellama3\u003c/code\u003e, \u003ccode\u003emistral\u003c/code\u003e, etc.).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eLOCAL_LLM_URI\u003c/code\u003e should point to the Ollama server, accessible from inside your Docker container via \u003ccode\u003ehost.docker.internal\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"how-to-build-your-docker-image\"\u003eHow to build your docker image\u003c/h1\u003e\n\u003cp\u003eSimply clone the repo and then build your own image like this :\u003c/p\u003e","title":"Startup pgAssistant with docker"},{"content":"myqueries.json file is used to store your helpfull queries.\nEach querie you add to the json file can be searched and executed by pgAssistant.\nThe JSON format is very simple :\n{ \u0026#34;id\u0026#34;: \u0026#34;db_version\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Database version\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Database\u0026#34;, \u0026#34;sql\u0026#34;: \u0026#34;SHOW server_version;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34; \u0026#34;reference\u0026#34;: \u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34; } id A unique ID of the query description The description of your SQL query categorie A SQL category like Database, Issue, Table, Index or whatever you want sql The SQL query ended with a \u0026ldquo;;\u0026rdquo; reference An URL on the query documentation or your project documentation type 2 sql types are alowed select : performing a select param_query : a select query with parameters. Each parameter must be in the format $1, $2, etc. ","permalink":"http://localhost:1313/doc/myqueries/","summary":"\u003cp\u003e\u003cstrong\u003emyqueries.json\u003c/strong\u003e file is used to store your helpfull queries.\u003c/p\u003e\n\u003cp\u003eEach querie you add to the json file can be searched and executed by pgAssistant.\u003c/p\u003e\n\u003cp\u003eThe JSON format is very simple :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-json\" data-lang=\"json\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;id\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;db_version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;description\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database version\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;category\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Database\u0026#34;\u003c/span\u003e,        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;sql\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;SHOW server_version;\u0026#34;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f92672\"\u003e\u0026#34;type\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;select\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;reference\u0026#34;\u003c/span\u003e: \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://www.postgresql.org/docs/current/sql-show.html\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eid\u003c/strong\u003e A unique ID of the query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003edescription\u003c/strong\u003e The description of your SQL query\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecategorie\u003c/strong\u003e A SQL category like Database, Issue, Table, Index or whatever you want\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esql\u003c/strong\u003e The SQL query ended with a \u0026ldquo;;\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ereference\u003c/strong\u003e An URL on the query documentation or your project documentation\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etype\u003c/strong\u003e 2 sql types are alowed\n\u003cul\u003e\n\u003cli\u003eselect : performing a select\u003c/li\u003e\n\u003cli\u003eparam_query : a select query with parameters. Each parameter must be in the format $1, $2, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Understanding the myqueries.json file"}]